# -*- coding: utf-8 -*-
"""keras_FPN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DRxzmyN9TFRZM8G_ZHQUikozx_13cYMd
"""

!pip install opencv-contrib-python==4.2.0.34

from __future__ import print_function
import numpy as np
import os
import time
import datetime
from matplotlib import pyplot as plt
import cv2
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models, applications
from tensorflow.keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model

import os
import sys

from google.colab import drive
drive.mount('/content/drive')

"""## Initialize GPU"""

gpus = tf.config.list_physical_devices('GPU')
if gpus:
  try:
    # Currently, memory growth needs to be the same across GPUs
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.list_logical_devices('GPU')
    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
  except RuntimeError as e:
    # Memory growth must be set before GPUs have been initialized
    print(e)

"""## Define plot functions and learning rate schedulers"""

def annot_max(x,y, ax=None):
    """ Annotate Maximum values of currently open plot
    """
    xmax = x[np.argmax(y)]
    ymax = y.max()
    text= "x={:.3f}, y={:.3f}".format(xmax, ymax)
    if not ax:
        ax=plt.gca()
    bbox_props = dict(boxstyle="square,pad=0.3", fc="w", ec="k", lw=0.72)
    arrowprops=dict(arrowstyle="->",connectionstyle="angle,angleA=0,angleB=60")
    kw = dict(xycoords='data',textcoords="data",
              arrowprops=arrowprops, bbox=bbox_props, ha="left", va="top")
    ax.annotate(text, xy=(xmax, ymax), xytext=(xmax-.025, ymax-.025), **kw)
    
def lr_schedule(epoch):
    """Learning Rate Schedule
    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.
    # Arguments
        epoch (int): The number of epochs
    # Returns
        lr (float32): learning rate
    """
    lr = 1e-3
    if epoch > 0.9*epoch:
        lr *= 0.5e-3
    elif epoch > 0.8*epoch:
        lr *= 1e-3
    elif epoch > 0.6*epoch:
        lr *= 1e-2
    elif epoch > 0.4*epoch:
        lr *= 1e-1
    else: 
        lr = 1e-3
    print('Learning rate: ', lr)
    return lr

"""## Define Models"""

def FPNRes50e100(image_shape, class_number):
    resnet50Backbone = get_backbone_ResNet50(input_shape=image_shape)
    model = customFeaturePyramid2(resnet50Backbone, class_number)
    return model

def FPNRes50V2e100(image_shape, class_number):
    resnet50V2Backbone = get_backbone_ResNet50V2(input_shape=image_shape)
    model = customFeaturePyramid2(resnet50V2Backbone, class_number)
    return model

def FPNRes101e200(image_shape, class_number):
    resnet101Backbone = get_backbone_ResNet101(input_shape=image_shape)
    model = customFeaturePyramid2(resnet101Backbone, class_number)
    return model

"""## Data preprocessing """

# data_path = "E:/Python/Image Inpainting"
data_path = "/content/drive/MyDrive/Graduation Project/saved_data/"
categories = ['NS', 'Original']#, 'PS', 'SM'
training_data = []
IMG_SIZE = 256

from skimage import feature

    
def local_binary_pattern(image):
    radius = 2
    n_points = 8 * radius
    img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    img_lbp = feature.local_binary_pattern(img_gray, n_points,radius, method="uniform")
    
    return img_lbp


def laplacian(src, ddepth = cv2.CV_16S, kernel_size = 3):
    # [reduce_noise]
    # Remove noise by blurring with a Gaussian filter
    src = cv2.GaussianBlur(src, (3, 3), 0)
    # [reduce_noise]
    # [convert_to_gray]
    # Convert the image to grayscale
    src_gray = cv2.cvtColor(src, cv2.COLOR_RGB2GRAY)
    # [convert_to_gray]
    # [laplacian]
    # Apply Laplace function
    dst = cv2.Laplacian(src_gray, ddepth, ksize=kernel_size)
    # [laplacian]
    # [convert]
    # converting back to uint8
    abs_dst = cv2.convertScaleAbs(dst)
    # [convert]
    return abs_dst


def add_channels(image, lbp, laplace):
    lbp = np.array(lbp).astype(np.uint8)
#     r,g,b = cv2.split(image)
#     print(image.dtype)
#     print(lbp.dtype)
#     print(laplace.dtype)
#     print(r.dtype)
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    merged = cv2.merge([gray,laplace,lbp])
    return merged

def create_training_data():
    for category in categories:
        print("Original" if category == 'Original' else "Inpaited")
        class_num = 0 if category == 'Original' else 1  # get the classification. 1=Inpainted 0=Original
        path = os.path.join(data_path, category)
        for image_path in os.listdir(path):
            image = cv2.imread(os.path.join(path,image_path))
            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            lbp = local_binary_pattern(image)
            laplace = laplacian(image)
            image = add_channels(image, lbp, laplace)
            training_data.append([image, class_num])  # add this to our training_data
            print(len(training_data))

# create_training_data()

# np.save("NS_TrainingData", training_data)

training_data = np.load("/content/drive/MyDrive/Graduation Project/NS_TrainingData.npy", allow_pickle=True)

import random

random.shuffle(training_data)

features = []  # X
labels = []    # y

for feature, label in training_data:
    features.append(feature)
    labels.append(label)

features = np.array(features).reshape(-1, IMG_SIZE, IMG_SIZE, 3)
print(features[0].shape)
print(labels[1])
plt.imshow(features[1])
plt.show()

for i in range(10):
    print(training_data[i][1])
    plt.imshow(training_data[i][0])
    plt.show()

from sklearn.model_selection import train_test_split

batch_size = 10
epochs = 5
########## Specify models to train and test
model_list = [FPNRes101e200] #FPNRes50e100
num_classes = 1

# X_train, X_test, y_train, y_test
train_img, test_img, train_lab, test_lab = train_test_split(features, labels, test_size=0.25) #load dataset

train_lab = np.array(train_lab).astype(np.int32)
test_lab = np.array(test_lab).astype(np.int32)

#train_img, test_img = train_img/255.0, test_img/255.0 #normalize dataset
# convert label to binary encoding
# train_lab = keras.utils.to_categorical(train_lab, num_classes)
# test_lab = keras.utils.to_categorical(test_lab, num_classes)
# val_img  = train_img[40000:]
# val_lab = train_lab[40000:]
# train_img = train_img[0:40000]
# train_lab = train_lab[0:40000]
# print(train_img.shape,test_img.shape)
img_shape = train_img.shape[1:]
img_rows = train_img[0].shape[0]
img_cols = train_img[0].shape[1]

print(img_shape)
print(img_rows)
print(img_cols)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models, applications


def get_backbone_ResNet50(input_shape):
    """Builds ResNet50 with pre-trained imagenet weights"""
    backbone = keras.applications.ResNet50(
        include_top=False, input_shape=input_shape
    )
    c3_output, c4_output, c5_output = [
        backbone.get_layer(layer_name).output
        for layer_name in ["conv3_block4_out", "conv4_block6_out", "conv5_block3_out"]
    ]
    return keras.Model(
        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]
    )


def get_backbone_ResNet50V2(input_shape):
    """Builds ResNet50 with pre-trained imagenet weights"""
    backbone = keras.applications.ResNet50V2(
        include_top=False, input_shape=input_shape
    )
    c3_output, c4_output, c5_output = [
        backbone.get_layer(layer_name).output
        for layer_name in ["conv3_block4_out", "conv4_block6_out", "conv5_block3_out"]
    ]
    return keras.Model(
        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]
    )


def get_backbone_ResNet101(input_shape):
    """Builds ResNet101 with pre-trained imagenet weights"""
    backbone = keras.applications.ResNet101(
        include_top=False, input_shape=input_shape
    )
    c3_output, c4_output, c5_output = [
        backbone.get_layer(layer_name).output
        for layer_name in ["conv3_block4_out", "conv4_block23_out", "conv5_block3_out"]
    ]
    return keras.Model(
        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]
    )


class customFeaturePyramid2(keras.models.Model):
    """Builds the Feature Pyramid with the feature maps from the backbone.

    Attributes:
      num_classes: Number of classes in the dataset.
      backbone: The backbone to build the feature pyramid from.
        Currently supports ResNet50, ResNet101 and V1 counterparts.
    """

    def __init__(self, backbone=None, class_number=2, **kwargs):
        super(customFeaturePyramid2, self).__init__(name="customFeaturePyramid2", **kwargs)
        self.backbone = backbone if backbone else get_backbone_ResNet50()
        self.class_number = class_number
        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, "same")
        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, "same")
        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, "same")
        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, "same")
        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, "same")
        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, "same")
        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, "same")
        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, "same")
        self.upsample_2x = keras.layers.UpSampling2D(2)
        self.dense_d1 = keras.layers.Dense(64,
                                           activation='relu',
                                           kernel_initializer='he_uniform')
        self.dense_d2 = keras.layers.Dense(self.class_number,
                                           activation='sigmoid',
                                           kernel_initializer='he_normal')

    def call(self, images, training=False):
        c3_output, c4_output, c5_output = self.backbone(images, training=training)
        p3_output = self.conv_c3_1x1(c3_output)
        p4_output = self.conv_c4_1x1(c4_output)
        p5_output = self.conv_c5_1x1(c5_output)
        p4_output = p4_output + self.upsample_2x(p5_output)
        p3_output = p3_output + self.upsample_2x(p4_output)
        p3_output = self.conv_c3_3x3(p3_output)
        p4_output = self.conv_c4_3x3(p4_output)
        p5_output = self.conv_c5_3x3(p5_output)
        p6_output = self.conv_c6_3x3(c5_output)
        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))
        p3_output = keras.layers.Flatten()(p3_output)
        p4_output = keras.layers.Flatten()(p4_output)
        p5_output = keras.layers.Flatten()(p5_output)
        p6_output = keras.layers.Flatten()(p6_output)
        p7_output = keras.layers.Flatten()(p7_output)
        m1_output = keras.layers.Concatenate(axis=1)([p3_output,
                                                      p4_output,
                                                      p5_output,
                                                      p6_output,
                                                      p7_output])
        m1_output = keras.layers.Flatten()(m1_output)
        m1_output = self.dense_d1(m1_output)
        m1_output = self.dense_d2(m1_output)
        return m1_output

test_loss = []
test_acc = []
time_taken = []
# opt = 'adam'
# loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)
opt = Adam(learning_rate = lr_schedule(epochs))
loss = 'binary_crossentropy'

for f in model_list:
    model = f(img_shape, num_classes)
#     model.build(img_shape)
#     model.summary()
    model.load_weights('/content/drive/MyDrive/Graduation Project/NSFPNRes101e200_epoch5_batch20_weights')
    model.compile(optimizer=opt,
                  loss=loss,
                  metrics=['accuracy'])
    history = model.fit(train_img,
                            train_lab,
                            batch_size=batch_size,
                            epochs=epochs,
                            validation_data=(test_img, test_lab),
                            shuffle=True)
    ########## save model
    #https://stackoverflow.com/questions/51806852/cant-save-custom-subclassed-model
    model.save_weights('/content/drive/MyDrive/Graduation Project/NS{}_epoch{}_batch{}_weights'.format(f.__name__, epochs, batch_size), save_format='tf')
    
    ########## plot stuff
    plt.style.use('seaborn')
    x = [i for i in range(1, epochs+1)]
    fig = plt.figure()
    ########## plot model training accuracy
    plt.subplot(211)
    acc = np.array(history.history['accuracy'])
    valAcc = np.array(history.history['val_accuracy'])
    plt.plot(acc)
    plt.plot(valAcc)
    annot_max(x, acc)
    annot_max(x, valAcc)
    plt.legend(['train', 'test'], loc='lower right')
    plt.title('{}_trainAcc'.format(f.__name__))
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    ########## plot model training loss
    plt.subplot(212)
    loss = np.array(history.history['loss'])
    valLoss = np.array(history.history['val_loss'])
    plt.plot(loss)
    plt.plot(valLoss)
    annot_max(x, loss)
    annot_max(x, valLoss)
    plt.legend(['train', 'test'], loc='lower right')
    plt.title('{}_trainLoss'.format(f.__name__))
    plt.ylabel('loss')
    plt.xlabel('epoch')

    plt.savefig('{}_trainAccLoss'.format(f.__name__), format='png')
    plt.close(plt.gcf())

    time1 = time.time()
    a, b = model.evaluate(test_img, test_lab, verbose=2)
    time_taken.append(time.time() - time1)
    test_loss.append(a)
    test_acc.append(b)

print(test_loss)
print(test_acc)
print(time_taken)
model.summary()